---
title: 'Estimador MCO'
author: "Andrés Vargas"
date: "12/02/2022"
output: rmdformats::downcute
bibliography: refeconometria.bib
link-citations: true
---

```{r setup_prog, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(
  comment = NA, 
  warning = FALSE, 
  message = FALSE
    )
library(fontawesome)
here::i_am("Notas de clase/02-mco.Rmd")
library(here)
here()
```

# Objetivo de la sesión

Estudiar las propiedades estadísticas del estimador MCO, a partir de lo cual se identifican las circunstancias bajo las cuales la estimación con el conjunto particular de datos que tenemos produce estimaciones que podemos estimar de forma causal


# Introducción

Queremos saber si las personas que tienen seguro médico gozan de una mejor salud aquellos que no tienen seguro médico. AL fin y al cabo si tiene seguro tiene unas condiciones de acceso a servicios de salud más favorables. Para probar esta hipótesis tiene datos que provienen de una muestra aleatoria con datos del estado de salud de cada individuo y si está o no asegurado. Como el estado de salud varía entre individuos, usted compara la media de la salud de los que tienen seguro con la media de la salud de aquellos que no lo tienen, esto es

\begin{equation}
E(Salud|Seguro=1)-E(Salud|Seguro=0)
\end{equation}

Si la función de expectativa condicional es lineal, entonces lo anterior lo podemos estudiar a partir del siguiente modelo

\begin{equation}
Salud=\alpha+\beta Seguro+u
\end{equation}

La tabla siguiente, tomada de @mastering, muestra un resumen de los datos con los que cuenta

```{r, echo=FALSE, fig.align="center",out.width="80%", fig.cap="Características de asegurados y no asegurados, tabla 1.1 de Angrist y Pischke (2014)"}
knitr::include_graphics(here("img","health.PNG"))
```

Si vemos la primera línea para los esposos vemos que $E(Salud|Seguro=1)=4.01$ y $E(Salud|Seguro=0)=3.7$, luego si hacemos la diferencia encontramos que

\begin{equation}
E(Salud|Seguro=1)-E(Salud|Seguro=1)=\beta=0.31
\end{equation}

Luego podriamos concluir que tener seguro médico mejora el estado de salud en promedio 0.31 puntos ¿Será que es correcta esa interpretación? ¿Podemos atribuir la mejor salud a la tenencia del seguro médico?

Sabemos que dicha interpretación sería correcta si $E(u|Seguro)=0$, en otras palabras que el error no guarda ninguna relación con la tenencia del seguro. Para saberlo, es importante recordar que $u$ recoge la variabilidad aleatoria en $Salud$, así como la indicencia de otras variables que son importantes para explicar el estado de salud pero que no se han incorporado de manera explícita en el modelo. Por ejemplo, se ha documentado que en general las personas de ingresos más bajos y/o en situación de desempleo tienden a tener un peor estado de salud debido al estrés y las restricciones de acceso a alimentos frescos, por ejemplo. Además, es más probable que esas personas no tengan seguro médico. Lo anterior nos lleva a pensar que $E(u|Seguro)\neq0$. 

Un mejor análisis tendría en cuenta estas variables^[Estoy suponiendo que no hay nada más que importe], luego se plantearía el siguiente modelo

\begin{equation}
Salud=\alpha+\beta Seguro + \gamma_1 Empleo+\gamma_2Ingreso+e
\end{equation}

Al sacar estas dos variables del error $u$ y colocarlas de manera explícita en el modelo tenemos que la relación entre $Seguro$ y el error desaparece, se cumple el suspuesto de de indepencia condicional, CIA, y por lo tanto en este caso $\beta$ si recoge el efecto causal del aseguramiento en la salud. 

Hasta ahora hemos dado vueltas sobre lo mismo, pro no hemos abordado la cuestión de cómo obtener el valor de $\beta$ con el conjunto particular de datos que tenemos. Esto nos lleva a la regresión lineal y el estimador de mínimos cuadrados ordinarios

# Regresión lineal y estimador MCO

Suponga que usted tiene los siguientes datos

---

```{r,echo=FALSE}
#Modelo simulado 2 variable

N<-100
a<-100
b1<-10
sig2e<-2500 #varianza del error
sde<-sqrt(sig2e) #desviación estándar del error

#generamos datos artificiales
set.seed(12345) #para iniciar la obtención de números aleatorios
x<-runif(n=N,min=1,max=25)
set.seed(12345) #para iniciar la obtención de números aleatorios
e<-rnorm(N,mean=0,sd=sde)

y<-a+b1*x+e #creamos la variable y
df<-data.frame(y,x) #creamos base de datos
df$lr1<-80+5*df$x
df$lr2<-100+12*df$x
df$lr3<-60+10*df$x
df$lr4<-100+10*df$x

library(ggplot2)
lr_plot<-ggplot(df,aes(x=x,y=y))+geom_point()+theme_minimal()
lr_plot+labs(title=expression(Y %~% X))

```

---

La regresión lineal no es más que buscar la línea que mejor se ajuste a estos datos. La forma más simple de estimar los parámetros es usando el estimador de Mínimos Cuadrados Ordinarios, MCO. La idea es la siguiente. 

Planteamos una relación lineal entre $y$ y $x$

\begin{equation}
y=\alpha+\beta x+e
\end{equation}

En esta ecuación $\alpha$ es el intercepto y $\beta$ la pendiente. El término $e$ lo llamamos el error. Note que el error recoge la diferencia entre lo que observamos de $y$ y lo que $x$ predice que será $y$. Si $\alpha=100$ y $\beta=10$ entonces podemos decir que si $x=10$ entonces $y=200$. Sin embargo, al observar la gráfica se dará cuenta que hay muchas puntos donde $x=10$ pero $y\neq200$. Esa diferencia es $e$ ¿Por qué podría darse esa diferencia entre lo predicho y lo observado? Varias razones

- Hay otras variables, ej. $z$, que pueden afectar el comportamiento de $y$, y que no hemos incluido

- Variabilidad aleatoria. 

Digamos que $\hat{y}$ es la línea de regresión, y esta es igual a 

\begin{equation}
\hat{y}=\hat{\alpha}+\hat{\beta}x
\end{equation}

La distancia entre cada punto y la línea de regresión es

\begin{equation}
\hat{e}_i=y_i-\hat{y}_i=y_i-\hat{\alpha}-\hat{\beta}x_i
\end{equation}

Los parámetros que producen la mejor línea son aquellos que minimizan la suma de los residuales al cuadrado

\begin{equation}
SSE=\sum_i\hat{e}_i^2
\end{equation}

Lo anterior quiere decir, que aún cuando hay muchas líneas que recogen la relación positiva entre nuestras variables, hay una que es la mejor de todas

---

```{r,echo=FALSE, warning=FALSE,message=FALSE}
library(ggpubr)
lr1_plot<-ggplot(df,aes(x=x))+geom_point(aes(y=y))+geom_line(aes(y=lr1,colour="lr1"))+theme_minimal()+
  theme(legend.position = "none")+scale_color_manual("",breaks=c("lr1"),values=c("lr1"="blue"))
lr2_plot<-ggplot(df,aes(x=x))+geom_point(aes(y=y))+geom_line(aes(y=lr2,colour="lr2"))+theme_minimal()+
  theme(legend.position = "none")+scale_color_manual("",breaks=c("lr2"),values=c("lr2"="red"))
lr3_plot<-ggplot(df,aes(x=x))+geom_point(aes(y=y))+geom_line(aes(y=lr3,colour="lr3"))+theme_minimal()+
  theme(legend.position = "none")+scale_color_manual("",breaks=c("lr3"),values=c("lr3"="green"))
lr4_plot<-ggplot(df,aes(x=x))+geom_point(aes(y=y))+geom_line(aes(y=lr4,colour="lr4"))+theme_minimal()+
  theme(legend.position = "none")+scale_color_manual("",breaks=c("lr4"),values=c("lr4"="#FF9900"))
lines<-ggarrange(lr1_plot,lr2_plot,lr3_plot,lr4_plot,nrow=2,ncol=2,
                 labels=c("A","B","C","D"))
annotate_figure(lines,top="¿Cuál prefiere?")

```

---

El estimador MCO, puede entenderse como un algoritmo para encontrar la mejor línea entre todas las posibles. Donde mejor significa la que minimiza la suma de residuales al cuadrado. En términos más simples, la línea que se equivoca menos.


# Las propiedades estadísticas del estimador MCO

Recuerde los puntos 2 y 3 mencionados por @heckman2008econometric

2. Identificar modelos causales de una población idealizada. Esto es, como si tuvieramos muestras infinitas sin variabilidad muestral

3. Identificar modelos causales a partir de los datos, donde la variabilidad muestral es un asunto

El punto 2 nos dice que hay una situación *ideal* en la que los parámetros del modelo recogen la relación causal. Digamos que son algo así como los parámetros verdaderos. Sin embargo, punto 3, nosotros nunca tenemos acceso a ese ideal, solo nos podemos aproximar a él a través del conjunto limitado de datos que tenemos. Nuestros datos son una muestra de las muchas muestras posibles que pudimos haber observado, luego debemos reconocer que si hubiésemos tenido otra muestra nuestros calculos habrían sido diferentes. Variabilidad muestral.

Nuestro punto de partida es el modelo poblacional

\begin{equation}
\tag{1}
y=\beta_1+\beta_2x_2+\beta_3x_3+...+\beta_kx_k+u
\end{equation}

Donde donde las variables $y,x_2,...,x_k$ son aleatorias y observables, y $u$ es un error no observable. Los parámetros $\beta_1,\beta_2,...,\beta_k$ son los que queremos estimar. El error $u$ recoge perturbaciones aleatorias, y también todo aquello que es importante para explicar $y$ pero que no hemos incluido explícitamente en el modelo, es decir *variables omitidas*. 

La idea de población no hace referencia, necesariamente, a una población física en el mundo real. Significa que si tenemos una observación para el individuo i, $(y_i,x_i)$, esta la consideramos como la realización de una función de probabilidad conjunta $F(y,x)$. Nosotros no conocemos $F$, y el propósito de la inferencia es aprender sus características a partir de una muestra, es decir del conjunto particular de datos que tenemos. 

Lo anterior significa que a partir de nuestros datos estimamos los valores de $\boldsymbol{\beta}$, y a estos los llamamos $\hat{\boldsymbol{\beta}}$ El estimador MCO consiste en estimar dichos parámetros a partir de encontrar el valor de ellos tales que se minimiza la diferencia al cuadrado entre el valor observado y el valor predicho, con una muestra particular de datos. Esto quiere decir que son aquellos que minimizan la expresión

\begin{equation}
\tag{2}
\sum_i^n(y_i-\hat{\beta_1}-\hat{\beta_2}x_{i2}-...-\hat{\beta_k}x_{ik})^2
\end{equation}

Donde $i=1,...,n$ identifica cada observación en la muestra. Al tomar las condiciones del primer orden obtenemos

\begin{align}
\tag{3}
\sum_i^n(y_i-\hat{\beta_1}-\hat{\beta_2}x_{i2}-...-\hat{\beta_k}x_{ik})&=0\\
\sum_i^nx_{i2}(y_i-\hat{\beta_1}-\hat{\beta_2}x_{i2}-...-\hat{\beta_k}x_{ik})&=0\\
.&\\
.&\\
.&\\
\sum_i^nx_{ik}(y_i-\hat{\beta_1}-\hat{\beta_2}x_{i2}-...-\hat{\beta_k}x_{ik})&=0
\end{align}

Fíjese que tenemos un sistema de $k$ ecuaciones con $k$ incognitas. En términos matriciales esto lo podemos escribir como

\begin{equation}
\tag{4}
\mathbf{X'}(\mathbf{y}-\mathbf{X}\boldsymbol{\hat{\beta}})=0
\end{equation}.

Donde $\mathbf{X}$ es $n\times k$ y recoje los datos de las variables independientes, mientras que $\mathbf{y}$ es $n\times 1$ y contiene los valores de la variable dependiente, y \boldsymbol{\hat{\beta}} es la matriz de parámetros estimados, de dimensión $k\times 1$.

La expresión anterior es equivalente a

\begin{equation}
\tag{5}
\mathbf{(X'X)}\hat{\boldsymbol{\beta}}=\mathbf{X'y}
\end{equation}

Si la matriz $\mathbf{(X'X)}$ es invertible entonces podemos premultiplicar a ambos lados por $\mathbf{(X'X)}^{-1}$ y obtenemos

\begin{equation}
\tag{6}
\hat{\boldsymbol{\beta}}=\mathbf{(X'X)}^{-1}\mathbf{X'y}
\end{equation}

La matriz $\mathbf{(X'X)}$ es invertible si no hay colinealidad perfecta entre las variables. 

Como el valor estimado de los parámetros se obtuvo de una muestra particular de datos entonces debemos tener en cuenta que pudimos haber observado una muestra diferente, con la cual el valor puntual estimado habría sido diferente. Nuestro objetivo es obtener las propiedades estadísticas del estimador

## El valor esperado del estimador MCO

- **S1** Modelo poblacional 

\begin{equation}
y=\beta_1+\beta_2x_2+\beta_3x_3+...+\beta_kx_k+u
\end{equation}

- **S2** Tenemos una muestra aleatoria de tamaño $n$, $\{(x_{i1},x_{i2},...,x_{ik}):i=1,2,...,n\}$, es decir que las observaciones son independientes e identicamente distribuidas. Por ejemplo, el ingreso y nivel educativo del individuo $i$ es independiente del individuo $j$. 

- **S3** No hay colinealidad perfecta, y por lo tanto $\mathbf{(X'X)}$ es invertible

- **S4** $E(u|x_1,x_2,...,x_k)=0$ El valor esperado condicional del error es cero. Es decir que el error no está relacionado con las variables independientes. 

Bajo estos supuestos, podemos mostrar que 

\begin{equation}
\tag{7}
E(\hat{\boldsymbol{\beta}}|\mathbf{X})=\boldsymbol{\beta}
\end{equation}

Veamos. Primero tomemos el valor esperado condicional en la ecuación $(6)$

\begin{equation}
\tag{8}
E(\hat{\boldsymbol{\beta}}|\mathbf{X})=\mathbf{(X'X)}^{-1}E(\mathbf{X'y}|\mathbf{X})
\end{equation}

Como $\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+u$, entonces

\begin{equation}
\tag{9}
E(\hat{\boldsymbol{\beta}}|\mathbf{X})=\mathbf{(X'X)}^{-1}\mathbf{X'X}\boldsymbol{\beta}+\mathbf{(X'X)}^{-1}\mathbf{X'}E(u|\mathbf{X})
\end{equation}

Luego, si se cumple **S4**

\begin{equation}
\tag{10}
E(\hat{\boldsymbol{\beta}}|\mathbf{X})=\boldsymbol{\beta}
\end{equation}

Es decir que el estimador es insesgado. 

## La varianza del estimador en datos de corte transversal

**S5** Homocedasticidad, $Var(u|\mathbf{X})=\sigma^2$. Es decir que la varianza condicional del error es la misma para todos los valores de las variables explicativas. 

Con *S5* entonces podemos mostrar que 

\begin{equation}
\tag{11}
Var(\hat{\boldsymbol{\beta}}|\mathbf{X})=\sigma^2\mathbf{(X'X)}^{-1}
\end{equation}

Para entenderlo mejor, la varianza para un $\beta_j$ particular sería

\begin{equation}
Var(\hat{\beta_j}|\mathbf{X})=\dfrac{\sigma^2}{\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2(1-R_j^2)}
\end{equation}

Donde $R_j^2$ es el $R$-cuadrado de una regresión de $x_j$ contra las demás independiente. Entre más correlacionada esté $x_j$ con las demás variables, mayor será el $R$-cuadrado


La varianza depende de tres cosas

- La varianza del error $\sigma^2$. Esto es una característica de la población. Si se agregan más variables esta podría reducirse. Sin embargo, si el modelo ya incluye las variables relevantes, entonces ya no habría nada que agregar

-  La variabilidad muestral de $x_j$: $\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2$ Entre mayor sea la variabilidad menor es la varianza. Al aumentar el tamaño de muestra la variabilidad se incrementa y disminuye la varianza del estimador

- El grado de relación lineal entre las variables independientes: $R_j^2$. Entre mayor sea la correlación la varianza es más grande. Una alta correlación siginifica que a pesar de tener muchos datos tengo poca información

## Error estándar e inferencia

Como $\sigma^2$ no es observable $Var(\hat{\beta_j}|\mathbf{X})$ no es computable. Para ello debo tener un estimador insesgado de $\sigma^2$, esto es un $\hat{\sigma}^2$ tal que $E(\hat{\sigma}^2)=\sigma^2$, y por lo tanto que tengamos un estimador insesgado de la varianza del estimador

Como $\sigma^2=E(u^2)$, entonces un estimador es la media muestral, promedio, de los residuales

\begin{equation}
\hat{\sigma}^2=\dfrac{\sum_{i=1}^n\hat{u}_i^2}{n-k}
\end{equation}

Luego el error estándar es

\begin{equation}
\tag{12}
se(\hat{\beta})=\dfrac{\hat{\sigma}}{\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2(1-R_j^2)}
\end{equation}


Importante. La formula anterior es válida bajo el supuesto de homocedasticidad

El próximo paso es hacer inferencia estadística, es decir la realización de pruebas de hipótesis. Para ello necesitamos la distribución muestral de $\hat{\beta_j}$. De la ecuación $(9)$ es claro que la distribución muestral, condicionada en las independientes, depende del error. 

- **S5** El error se distribuye normal con media cero y varianza $\sigma^2$: $u\sim N(0,\sigma^2)$

Bajo los supuestos anteriores y **S5**, tenemos entonces que

\begin{equation}
\tag{13}
\hat{\beta}_j\sim N(\beta,Var(\hat{\beta}))
\end{equation}

Luego

\begin{equation}
\tag{14}
\dfrac{\hat{\beta}_j-\beta_j}{sd(\hat{\beta}_j)}\sim N(0,1)
\end{equation}

Para hacer pruebas de hipótesis sobre un solo parámetro usamos $(14)$ pero teniendo en cuenta que $sd(\hat{\beta})$ no es observable, pero su estimación es el error estándar, de donde tenemos que

\begin{equation}
\tag{15}
\dfrac{\hat{\beta}_j-\beta_j}{se(\hat{\beta}_j)}\sim t_{n-k}
\end{equation}

Ahora, para probar $H_0:\beta_j=0$ usamos la estadística $t\equiv(\hat{\beta_j}-\beta_{j,H_0})/se(\hat{\beta}_j)$. Esta me dice que tanto se desvia el valor estimado del valor bajo la hipótesis nula en relación a la desviación estándar. Por ejemplo, si $t=1$ decimos que el valor estimado es mayor a cero en una desviación estándar del estimador. Dado que se obtiene un valor puntual de $\hat{\beta_j}$, pero sabemos que pudimos haber obtenido un valor diferente con otra muestra, entonce debemos examinar la distribución de $\hat{\beta}_j$ para saber que tan probable es que hubiésemos obtenido un valor estimado de cero. La prueba $t$ me permite responder esa pregunta

### Demostración

Vamos a simular unos datos $Y=\alpha+\beta_{X1}X_1+\beta_{X2}X_2+u$. Empezaremos simulando un proceso con $\alpha=5$, $\beta_{X1}=0.5$, $\beta_{X2}=3$. Tomamos inicialmente una muestra $n\approx 100$, y estimamos los coeficientes vía MCO

<br/>

1. Simulamos los datos y tomamos una muestra

<br/>

```{r}
library(mvtnorm)
library(ggplot2)
N<-10000
coefs<-cbind("hat_beta_1" = numeric(1000), "hat_beta_2" = numeric(1000)) #Vector que guardará los coeficientes
set.seed(1) # permite reproducir los resultados
X <- rmvnorm(N, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10))) # generamos X1 y X2
u <- rnorm(N, sd = 5)
Y <- 5 + 0.5 * X[, 1] + 3 * X[, 2] + u

xdf<-data.frame(X,Y)
dfs<-sample(c(TRUE,FALSE),nrow(xdf),replace=TRUE,prob=c(0.01,0.99)) #muestra aleatoria, n aprox 100
dfs<-xdf[dfs,]

```

<br/>

2. Estimamos por MCO

```{r}
model1<-lm(Y~X1+X2,data=dfs)
model1

```

<br/>

3. Calculamos el error estándar y el estadístico $t$. Tenga en cuenta que para estimar la varianza del estimador necesitamos $$\hat{\sigma}^2=\dfrac{\sum_{i}^{n}\hat{u}_i}{n-k}$$

```{r}
X1bar<-mean(dfs$X1)
sumX1sqr<-sum((dfs$X1-X1bar)^2)
Rsqrx1<-summary(lm(X1~X2,data=dfs))$r.squared
uhat<-model1$residuals
sigmahat<-(sum(uhat^2))/(model1$df.residual)#Varianza estimada del error
varhatb1hat<-sigmahat/(sumX1sqr*(1-Rsqrx1))#Varianza estimada del estimador
se<-sqrt(varhatb1hat)
se
```


```{r}
t<-coef(model1)[2]/se
t
```

<br/>

4. Hacemos la prueba de hipótesis $Ho:\beta_{X1}=0$ contra la alternativa $Ha:\beta_{X1}\neq0$. Grafiquemos la distribución $t_{df}$ con los grados de libertar correspondientes y veamos donde se ubica nuestro estadístico $t$

```{r}
funcShaded <- function(x) {
  y <- dt(x,df=87)
  y[x > -2&x<2 ] <- NA
  return(y)
}

tdst<- ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = model1$df.residual))
tdst+theme_classic()+stat_function(fun=funcShaded,geom="area", fill="blue",alpha=0.2)+
  annotate("text", x =-3 , y = 0.1,label="Area=0.25")+annotate("text", x =3 , y = 0.1,label="Area=0.25")+
labs(title="Zonas de rechazo al 5%, df=60",y="",x="")


```

## Propiedades asintóticas

¿Qué pasa con el estimador en la medida que nuestra muestra aumenta? 

